{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rS7cmjl4uvrt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.What is a parameter\n",
        "\n",
        "- parameter typically refers to the internal variables of a model that are learned from the training data. These are the values the model adjusts during the training process to better fit the data and make accurate predictions."
      ],
      "metadata": {
        "id": "aW2l-tppu8KM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What is correlation?\n",
        "What does negative correlation mean\n",
        "\n",
        "-Correlation is a statistical term that describes the relationship or connection between two or more variables. It shows how changes in one variable might correspond to changes in another.\n",
        "\n",
        "-Negative Correlation:\n",
        "\n",
        "  When one variable increases, the other variable tends to decrease (and vice versa).\n",
        "    Example: As the number of hours spent watching TV increases, the number of hours spent studying might decrease. This shows a negative correlation.\n",
        "\n"
      ],
      "metadata": {
        "id": "X9LVcxMevKYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Define Machine Learning. What are the main components in Machine Learning\n",
        "\n",
        "-Machine Learning (ML) is a field of artificial intelligence (AI) that enables systems to automatically learn and improve from experience without being explicitly programmed. It involves creating algorithms and models that can recognize patterns in data, make predictions, or decisions based on that data, and continuously improve as they are exposed to more data.\n",
        "\n",
        "Main Components of Machine Learning:\n",
        "\n",
        "  - Data:\n",
        "       Data is the foundation of any ML model. It’s the information that the model uses to learn and make predictions or decisions. The data could come in many forms: numbers, images, text, etc.\n",
        "      High-quality data is crucial to building an accurate model.\n",
        "\n",
        "  -Algorithms:\n",
        "    Algorithms are the mathematical models or techniques that ML systems use to process data, learn from it, and make predictions or decisions.\n",
        "   \n",
        "\n",
        "  - Model:\n",
        "      A model is the trained version of an algorithm that has learned patterns from the data. Once an algorithm is applied to data and trained, it becomes a model capable of making predictions.\n",
        "       \n",
        "  - Features (or Input Variables):\n",
        "     Features are the individual measurable properties or characteristics of the data. For example, in predicting house prices, features might include the number of bedrooms, square footage, location, etc.\n",
        "        \n",
        "\n",
        "  -Training:\n",
        "        The training process is where the machine learning algorithm learns from the data. During training, the model adjusts its parameters (like weights and biases in a neural network) to reduce errors or improve predictions.\n",
        "        This step often involves optimizing a loss function, which measures how far the model’s predictions are from the actual results.\n",
        "\n",
        "  - Labels (in supervised learning):\n",
        "    In supervised learning, labels are the known outcomes or results that the model tries to predict. For example, in a spam email classification task, the label might be \"spam\" or \"not spam\" for each email.\n",
        "\n",
        "   - Evaluation:\n",
        "        After training a model, it needs to be evaluated to assess its performance. This is done by testing the model on a separate dataset that it hasn’t seen before (often called the test set). Common evaluation metrics include:\n",
        "            Accuracy,\n",
        "            Precision,\n",
        "            Recall,\n",
        "            F1 score,\n",
        "            Mean squared error (MSE), depending on the type of problem (classification or regression).\n",
        "\n",
        "    Optimization:\n",
        "        Optimization refers to the process of improving the model by tweaking parameters or adjusting the learning process. This could involve techniques like:\n",
        "            Hyperparameter tuning (e.g., adjusting the learning rate, the number of layers in a neural network),\n",
        "            Cross-validation (testing the model on different subsets of the data to reduce overfitting).\n",
        "\n",
        "    Inference:\n",
        "        After a model is trained and optimized, inference refers to using the model to make predictions or decisions based on new, unseen data.\n",
        "\n"
      ],
      "metadata": {
        "id": "_-L9Lli1vhFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.How does loss value help in determining whether the model is good or not\n",
        "\n",
        "-the loss value helps assess whether the model is making accurate predictions and guides the training process towards improvement. If the model has a low loss, it is considered a good model for the task."
      ],
      "metadata": {
        "id": "eVXIhx0pju7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.What are continuous and categorical variables\n",
        "\n",
        "-    Continuous variables have an infinite number of possible values and are used for measurements and quantities.\n",
        "-    Categorical variables represent distinct groups or categories, which can either have no order (nominal) or a defined order (ordinal).\n",
        "\n"
      ],
      "metadata": {
        "id": "FxdMjBWxk88m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.How do we handle categorical variables in Machine Learning? What are the common technique\n",
        "\n",
        "-    Label Encoding: Best for ordinal data with a natural order.\n",
        "    One-Hot Encoding: Best for nominal data with no order\n",
        "\n",
        "    Ordinal Encoding: Best for ordinal data.\n",
        "\n",
        "    Frequency Encoding: Useful when categorical variables have many categories.\n",
        "\n",
        "    Target Encoding: Effective when there’s a correlation between the categorical variable and the target.\n",
        "\n",
        "    Binary Encoding: Useful when there are many categories, but you want to avoid high-dimensionality.\n",
        "    \n",
        "    Hashing: Good for datasets with a large number of categories.\n",
        "\n",
        "The choice of encoding technique depends on the nature of the categorical variable (whether it's ordinal or nominal) and the characteristics of the data (e.g., number of categories, target relationships)."
      ],
      "metadata": {
        "id": "lfDNGoqrmJ7R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.What do you mean by training and testing a dataset\n",
        "\n",
        "-Training a Dataset:\n",
        "\n",
        "What it is: Training a dataset refers to the process of feeding your data (features and corresponding labels or targets) into a machine learning model so that it can learn from the patterns in the data. This step involves using an algorithm to find the optimal parameters (weights, biases, etc.) that allow the model to make accurate predictions or classifications.\n",
        "\n",
        " Testing a Dataset:\n",
        "\n",
        " What it is: Testing a dataset involves evaluating the performance of the trained model on a separate set of data that the model has never seen before. This dataset is called the test set. Testing helps determine how well the model generalizes to new, unseen data and checks its performance after training."
      ],
      "metadata": {
        "id": "vZT7RFMsmYGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.What is sklearn.preprocessing\n",
        "\n",
        "-sklearn.preprocessing is a module in scikit-learn (a popular Python machine learning library) that provides various functions and classes to transform and scale data before applying machine learning algorithms. Data preprocessing is a crucial step in machine learning because raw data often needs to be transformed into a format that models can work with effectively. This module includes tools to standardize, normalize, encode, and scale your data.\n",
        "\n"
      ],
      "metadata": {
        "id": "l7kISWBbnbva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.What is a Test set?\n",
        "\n",
        "-A test set is a portion of your dataset that is used to evaluate the performance of a machine learning model after it has been trained. The key idea behind the test set is that it is data that the model has never seen before during the training process, so it allows you to assess how well the model generalizes to new, unseen data."
      ],
      "metadata": {
        "id": "lcen0g1onrMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?\n",
        "\n",
        "-    Understand the problem and gather data.\n",
        "    Explore and clean the data.\n",
        "\n",
        "    Preprocess the data (handle missing values, scale features, encode categorical variables).\n",
        "\n",
        "    Split the data into training and testing sets.\n",
        "\n",
        "    Choose a model based on the problem type.\n",
        "\n",
        "    Train the model using the training data.\n",
        "\n",
        "    Evaluate the model on the test data using appropriate metrics.\n",
        "    \n",
        "    Fine-tune the model if necessary.\n",
        "    Deploy and monitor the model.\n",
        "\n",
        "By following this workflow, you can systematically approach and solve any machine learning problem while ensuring robust and reliable models."
      ],
      "metadata": {
        "id": "65_h8OfJn6e9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "-EDA is a critical step for:\n",
        "\n",
        "   Understanding the dataset (structure, types, relationships).\n",
        "\n",
        "  Cleaning the data (handling missing values, outliers, inconsistencies).\n",
        "\n",
        "  Feature engineering (transformations, encoding, selection).\n",
        "\n",
        "  Guiding model choice by analyzing feature-target relationships.\n",
        "\n",
        "  Ensuring model assumptions are met and preparing data for machine learning algorithms.\n",
        "\n",
        "Without performing EDA, you're essentially blindfolding yourself and diving straight into model fitting without fully understanding the data’s nuances. EDA sets the foundation for a well-performing machine learning model by helping you make informed decisions throughout the modeling process.\n"
      ],
      "metadata": {
        "id": "_0GUqLsnoT3C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.What is correlation\n",
        "\n",
        "-Correlation refers to a statistical measure that describes the strength and direction of the relationship between two or more variables. In other words, it quantifies how changes in one variable are associated with changes in another variable."
      ],
      "metadata": {
        "id": "GNBHrLRQonJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.What does negative correlation mean?\n",
        "\n",
        "-Negative correlation means that when one variable increases, the other variable tends to decrease, and vice versa. Essentially, the two variables move in opposite directions.\n",
        "\n",
        "In simpler terms:\n",
        "\n",
        "   - As one variable goes up, the other goes down.\n",
        "\n",
        "   - As one variable goes down, the other goes up."
      ],
      "metadata": {
        "id": "huXIMFMwoxjU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.How can you find correlation between variables in Python?\n",
        "\n",
        "-    df.corr() (Pandas): Calculate correlation between numerical columns of a DataFrame\n",
        ".\n",
        "  -np.corrcoef() (NumPy): Compute the correlation coefficient between arrays or variables.\n",
        "\n",
        " -Heatmap (Seaborn): Visualize the correlation matrix with colors to easily identify patterns.\n",
        "\n",
        "By using these methods, you can quickly assess the relationships between variables in your dataset and choose the most appropriate modeling strategies."
      ],
      "metadata": {
        "id": "3m-R2fPjpAYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "-Causation refers to a cause-and-effect relationship between two variables, where one variable (the cause) directly influences or leads to a change in the other variable (the effect). In other words, one variable is responsible for bringing about a change in another.\n",
        "\n",
        "-Correlation: Two variables change together, but it doesn't mean one causes the other.\n",
        "\n",
        "Causation: One variable directly causes a change in another."
      ],
      "metadata": {
        "id": "pCoKnRiPpTrF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "-An optimizer in machine learning and deep learning is an algorithm or method used to adjust the weights of a model's parameters (e.g., weights in neural networks) in order to minimize or maximize an objective function, often referred to as the loss function or cost function. The goal is to reduce the error between the predicted outputs and the true outputs, leading to a model that generalizes well to unseen data.\n",
        "\n",
        "Types of Optimizers:\n",
        "\n",
        "- Gradient Descent (GD): Basic optimizer that updates parameters in the direction of the negative gradient.\n",
        "        Types:\n",
        "            Batch GD: Uses entire dataset for each update.\n",
        "            Stochastic GD (SGD): Uses one data point at a time.\n",
        "            Mini-batch GD: Uses a small batch of data points.\n",
        "\n",
        "   - Momentum:\n",
        "        Adds \"velocity\" from past gradients to accelerate convergence, reducing oscillations.\n",
        "\n",
        "   - Adagrad:\n",
        "        Adapts the learning rate for each parameter based on its frequency of updates. Good for sparse data.\n",
        "\n",
        "  - RMSprop: Adjusts the learning rate based on the moving average of squared gradients. Prevents learning rate decay like Adagrad.\n",
        "\n",
        "  - Adam (Adaptive Moment Estimation): Combines Momentum and RMSprop, tracking both the first moment (mean) and second moment (variance) of gradients for better adaptive learning.\n"
      ],
      "metadata": {
        "id": "066XceLspkgD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.What is sklearn.linear_model ?\n",
        "\n",
        "-sklearn.linear_model is a module in scikit-learn (a popular machine learning library in Python) that provides various linear models for regression and classification tasks. Linear models predict an output variable based on a linear combination of input features. These models are widely used because they are simple, fast, and interpretable."
      ],
      "metadata": {
        "id": "xo202fYYqOYz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.What does model.fit() do? What arguments must be given\n",
        "\n",
        "-The model.fit() method in scikit-learn is used to train the machine learning model on the provided dataset. It learns the relationship between the input features (X) and the target labels (y) by adjusting the model's parameters (such as weights and biases for linear models). Essentially, fit() allows the model to \"learn\" from the data."
      ],
      "metadata": {
        "id": "fLur61b6qVL-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.What does model.predict() do? What arguments must be given?\n",
        "\n",
        "-model.predict(X): Makes predictions on new data after the model has been trained.\n",
        "\n",
        " X: The new input data (features), in the same shape as the training data.\n",
        "\n",
        "\n",
        "Returns: Predicted labels (for classification) or continuous values (for regression), corresponding to the new input data."
      ],
      "metadata": {
        "id": "IuIuCPBRqhiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.What are continuous and categorical variables\n",
        "\n",
        "-Continuous variables are numerical and can take any value within a range, including fractions and decimals.\n",
        "Categorical variables represent categories and can be divided into distinct groups, without meaningful numeric values."
      ],
      "metadata": {
        "id": "jLk5a7rsq70H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "-Feature scaling is the process of normalizing or standardizing the range of independent variables (features) in a dataset. It transforms features so that they have similar magnitudes, making it easier for machine learning algorithms to interpret and perform optimally.\n",
        "\n",
        "Importance: Feature scaling is critical for many machine learning algorithms (like KNN, SVM, and Gradient Descent) to ensure proper model performance, faster convergence, and improved accuracy.\n"
      ],
      "metadata": {
        "id": "yKcjEaVGrHLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.How do we perform scaling in Python\n",
        "-feature scaling can be easily performed using the sklearn.preprocessing module from the scikit-learn library. Scikit-learn provides several classes to perform different types of scaling, such as Min-Max Scaling, Standardization (Z-score Scaling), and Robust Scaling.\n",
        "\n",
        "- Min-Max Scaling:\n",
        "   Use MinMaxScaler to scale data to a specific range [0, 1].\n",
        "- Standardization:\n",
        "    Use StandardScaler to standardize data (mean=0, standard deviation=1).\n",
        "- Robust Scaling:\n",
        "    Use RobustScaler to scale data using median and IQR, making it robust to outliers.\n",
        "\n",
        "These scaling techniques can be applied directly to your dataset using scikit-learn's built-in classes to ensure better model performance and faster convergence.\n"
      ],
      "metadata": {
        "id": "YvRVJFTkrWD5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.What is sklearn.preprocessing?\n",
        "\n",
        "-sklearn.preprocessing is a module in scikit-learn that provides various tools and functions for data preprocessing in machine learning. The goal of preprocessing is to prepare and transform your data into a format that is suitable for machine learning models. This module helps in tasks such as scaling, encoding, and transforming features to improve model performance."
      ],
      "metadata": {
        "id": "H2XJx2B6sXZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.How do we split data for model fitting (training and testing) in Python\n",
        "\n",
        "-you can split your data into training and testing sets using the train_test_split() function from scikit-learn. This function allows you to randomly divide your dataset into two parts: one for training the model and the other for testing its performance.\n",
        "\n",
        "train_test_split() is used to randomly split data into training and testing sets.\n",
        "\n",
        "The test_size argument defines the proportion of data for testing (commonly 0.2 or 0.3).\n",
        "\n",
        "You can use stratify=y to maintain class distribution in both sets (for classification problems).\n",
        "\n",
        "random_state ensures reproducibility of the data split."
      ],
      "metadata": {
        "id": "_CVTtBuZsoXi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25.Explain data encoding?\n",
        "\n",
        "-Data encoding is the process of converting categorical data (non-numeric) into numerical values, making it suitable for machine learning algorithms, which generally work better with numerical data. In machine learning, categorical data refers to variables that represent categories or classes, like \"Gender\" (Male/Female), \"Country\" (USA/India/Canada), or \"Color\" (Red/Blue/Green).\n",
        "\n"
      ],
      "metadata": {
        "id": "kGwR_J7CtAUj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fw5GCEQlvJG5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}